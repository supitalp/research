<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.56.1" />
  <meta name="author" content="Henri Rebecq">
  <meta name="description" content="PhD student">

  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/research/css/highlight.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.0/css/academicons.min.css" integrity="sha512-GGGNUPDhnG8LEAEDsjqYIQns+Gu8RBs4j5XGlxl7UfRaZBhCCm5jenJkeJL8uPuOXGqgl8/H1gjlWQDRjd3cUQ==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather%7CRoboto+Mono">
  <link rel="stylesheet" href="/research/css/hugo-academic.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-84780527-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  <link rel="alternate" href="https://supitalp.github.io/research/index.xml" type="application/rss+xml" title="Henri Rebecq">
  <link rel="feed" href="https://supitalp.github.io/research/index.xml" type="application/rss+xml" title="Henri Rebecq">

  <link rel="icon" type="image/png" href="/research/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/research/img/apple-touch-icon.png">

  <link rel="canonical" href="https://supitalp.github.io/research/">

  

  <title>Henri Rebecq</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/research/">Henri Rebecq</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/research/#about" data-target="#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/research/#publications_selected" data-target="#publications_selected">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/research/#teaching" data-target="#teaching">
            
            <span>Teaching</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/research/#contact" data-target="#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>



<span id="homepage" style="display: none"></span>



  
  
  <section id="about" class="home-section">
    <div class="container">
      



<div class="row" itemprop="author" itemscope itemtype="http://schema.org/Person" itemref="person-email person-address">
  <div class="col-xs-12 col-md-4">
    <div id="profile">

      
      <div class="portrait" style="background-image: url('https://supitalp.github.io/research/img/portrait.jpg');"></div>
      <meta itemprop="image" content="https://supitalp.github.io/research/img/portrait.jpg">
      

      <div class="portrait-title">
        <h2 itemprop="name">Henri Rebecq</h2>
        <h3 itemprop="jobTitle">PhD student</h3>
        
        <h3 itemprop="worksFor" itemscope itemtype="http://schema.org/Organization">
          <a href="http://rpg.ifi.uzh.ch" target="_blank" itemprop="url">
            <span itemprop="name">ETH Zürich / University of Zürich</span>
          </a>
        </h3>
        
      </div>

      <link itemprop="url" href="https://supitalp.github.io/research/">

      <ul class="social-icon" aria-hidden="true">
        
        
        <li>
          <a itemprop="sameAs" href="mailto:h.rebecq@gmail.com" target="_blank">
            <i class="fa fa-envelope big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://scholar.google.com/citations?user=zveWLBkAAAAJ" target="_blank">
            <i class="ai ai-google-scholar big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="https://www.linkedin.com/in/henri-rebecq-81bb9a97" target="_blank">
            <i class="fa fa-linkedin big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="//github.com/supitalp" target="_blank">
            <i class="fa fa-github big-icon"></i>
          </a>
        </li>
        
        
        <li>
          <a itemprop="sameAs" href="cv_rebecq.pdf" target="_blank">
            <i class="fa fa-paperclip big-icon"></i>
          </a>
        </li>
        
      </ul>

    </div>
  </div>
  <div class="col-xs-12 col-md-8" itemprop="description">

    

<h1 id="biography">Biography</h1>

<p>I am a PhD student at the <a href="http://rpg.ifi.uzh.ch" target="_blank">Robotics and Perception Group</a> directed by <a href="http://rpg.ifi.uzh.ch/people_scaramuzza.html" target="_blank">Davide Scaramuzza</a>, at ETH Zürich / University of Zürich.</p>

<p>I work on <a href="http://rpg.ifi.uzh.ch/docs/scaramuzza/2019.07.11_Scaramuzza_Event_Cameras_Tutorial.pdf" target="_blank">event cameras</a>, which are novel, bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very <strong>high dynamic range</strong>, <strong>no motion blur</strong>, and very <strong>low latency</strong> (in the order of microseconds).</p>

<p>My goal is to design novel algorithms that leverage the outstanding properties of event cameras to solve fundamental vision tasks, such as <a href="https://youtu.be/eomALySSGVU" target="_blank">high speed video</a>, <a href="https://youtu.be/jIvJuWdmemE" target="_blank">SLAM</a>, <a href="https://youtu.be/F3OFzsaPtvI" target="_blank">visual odometry</a> or <a href="https://www.youtube.com/watch?v=EUX3Tfx0KKE" target="_blank">3D reconstruction</a>.</p>

<p>I have graduated from <a href="https://www.telecom-paristech.fr/" target="_blank">Télécom ParisTech</a>, and I have studied <strong>machine learning</strong> and <strong>computer vision</strong> through the <a href="http://math.ens-paris-saclay.fr/version-francaise/formations/master-mva/" target="_blank">MVA M.Sc.</a> at ENS Cachan.</p>


    <div class="row">

      
      <div class="col-sm-5">
        <h3>Interests</h3>
        <ul class="ul-interests">
          
          <li>Event cameras</li>
          
          <li>Machine Learning</li>
          
          <li>SLAM</li>
          
          <li>3D Reconstruction</li>
          
          <li>Visual Odometry</li>
          
        </ul>
      </div>
      

      
      <div class="col-sm-7">
        <h3>Education</h3>
        <ul class="ul-edu fa-ul">
          
          <li>
            <i class="fa-li fa fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PhD student, 2015</p>
              <p class="institution">Robotics and Perception Group, University of Zürich</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fa fa-graduation-cap"></i>
            <div class="description">
              <p class="course">M.Sc. Mathematics, Vision and Learning, 2014</p>
              <p class="institution">Ecole Normale Supérieure de Cachan</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fa fa-graduation-cap"></i>
            <div class="description">
              <p class="course">M.Sc.Eng., 2014</p>
              <p class="institution">Télécom ParisTech</p>
            </div>
          </li>
          
        </ul>
      </div>
      

    </div>
  </div>
</div>

    </div>
  </section>

  
  
  <section id="publications_selected" class="home-section">
    <div class="container">
      



<div class="row">
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Selected Publications</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    

    
    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/e2vid_pami/">
        <img src="/research/img/banners/e2vid_pami.jpg" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/e2vid_pami/" itemprop="url">High Speed and High Dynamic Range Video with an Event Camera</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Event cameras are novel sensors that report brightness changes in the form of a stream of asynchronous events instead of intensity frames. They offer significant advantages with respect to conventional cameras: high temporal resolution, high dynamic range, and no motion blur. In this work we propose to learn to reconstruct intensity images from event streams directly from data instead of relying on any hand-crafted priors. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. We show that the network is able to synthesize high framerate videos (&gt; 5,000 frames per second) of high-speed phenomena (e.g. a bullet hitting an object) and is able to provide high dynamic range reconstructions in challenging lighting conditions.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Henri Rebecq, René Ranftl, Vladlen Koltun, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>T-PAMI&rsquo;20</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/e2vid_pami/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/TPAMI19_Rebecq.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/eomALySSGVU">
  Video
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/uzh-rpg/rpg_e2vid">
  Code
</a>





      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/ced_cvprw/">
        <img src="/research/img/banners/ced_cvprw.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/ced_cvprw/" itemprop="url">CED: Color Event Camera Dataset</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Until recently, event cameras have been limited to outputting events in the intensity channel, however, recent advances have resulted in the development of color event cameras, such as the Color DAVIS346. In this work, we present and release the first Color Event Camera Dataset (CED), containing 50 minutes of footage with both color frames and events. CED features a wide variety of indoor and outdoor scenes, which we hope will help drive forward event-based vision research. We also present an extension of the event camera simulator ESIM that enables simulation of color events. Finally, we present an evaluation of three state-of-the-art image reconstruction methods that can be used to convert the Color DAVIS346 into a continuous-time, HDR, color video camera to visualise the event stream, and for use in downstream vision applications.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Cedric Scheerlinck, Henri Rebecq, Timo Stoffregen, Nick Barnes, Robert Mahony, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>CVPR-W&rsquo;19</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/ced_cvprw/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/CVPRW19_Scheerlinck.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/R9BiRN7f7uY">
  Video
</a>



<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/CED.html">
  Dataset
</a>




      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/e2vid_cvpr/">
        <img src="/research/img/banners/e2vid_cvpr.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/e2vid_cvpr/" itemprop="url">Events-to-Video: Bringing Modern Computer Vision to Event Cameras</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        The output of event cameras is fundamentally different from conventional cameras, and it is commonly accepted that they require the development of specialized algorithms to accommodate the particular nature of events.  In this work, we take a different view and propose to apply existing, mature computer vision techniques to videos reconstructed from event data. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. Our experiments show that our approach surpasses state-of-the-art reconstruction methods by a large margin (&gt;20%) in terms of image quality.  We further apply off-the-shelf computer vision algorithms to videos reconstructed from event data on tasks such as object classification and visual-inertial odometry, and show that this strategy consistently outperforms algorithms that were specifically designed for event data.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Henri Rebecq, René Ranftl, Vladlen Koltun, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>CVPR&rsquo;19</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/e2vid_cvpr/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/CVPR19_Rebecq.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/IdYrC4cUO0I">
  Video
</a>






      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/fpv_icra19/">
        <img src="/research/img/banners/fpv_icra19_banner.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/fpv_icra19/" itemprop="url">Are We Ready for Autonomous Drone Racing? The UZH-FPV Drone Racing Dataset</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Despite impressive results in visual-inertial state estimation in recent years, high speed trajectories with six degree of freedom motion remain challenging for existing estimation algorithms. Aggressive trajectories feature large accelerations and rapid rotational motions, and when they pass close to objects in the environment, this induces large apparent motions in the vision sensors, all of which increase the difficulty in estimation. Existing benchmark datasets do not address these types of trajectories, instead focusing on slow speed or constrained trajectories, targeting other tasks such as inspection or driving. We introduce the UZH-FPV Drone Racing dataset, consisting of over 27 sequences, with more than 10 km of flight distance, captured on a first-person-view (FPV) racing quadrotor flown by an expert pilot. The dataset features camera images, inertial measurements, event-camera data, and precise ground truth poses. These sequences are faster and more challenging, in terms of apparent scene motion, than any existing dataset. Our goal is to enable advancement of the state of the art in aggressive motion estimation by providing a dataset that is beyond the capabilities of existing state estimation algorithms.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Jeffrey Delmerico, Titus Cieslewski, Henri Rebecq, Matthias Faessler, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>ICRA&rsquo;19</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/fpv_icra19/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/ICRA19_Delmerico.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/G5w4ZcEzvoo">
  Video
</a>






      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/photometric_tracking_icra19/">
        <img src="/research/img/banners/photometric_tracking_icra19_banner.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/photometric_tracking_icra19/" itemprop="url">Event-based, Direct Camera Tracking from a Photometric 3D Map using Nonlinear Optimization</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Event cameras are novel bio-inspired vision sensors that output pixel-level intensity changes, called &lsquo;events&rsquo;, instead of traditional video images. These asynchronous sensors naturally respond to motion in the scene with very low latency (in the order of microseconds) and have a very high dynamic range. These features, along with a very low power consumption, make event cameras an ideal sensor for fast robot localization and wearable applications, such as AR/VR and gaming. Considering these applications, we present a method to track the 6-DOF pose of an event camera in a known environment, which we contemplate to be described by a photometric 3D map (i.e., intensity plus depth information) built via classic dense 3D reconstruction algorithms. Our approach uses the raw events, directly, without intermediate features, within a maximum-likelihood framework to estimate the camera motion that best explains the events via a generative model. We successfully evaluate the method using both simulated and real data, and show improved results over the state of the art. We release the datasets to the public to foster reproducibility and research in this topic.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Samuel Bryner, Guillermo Gallego, Henri Rebecq, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>ICRA&rsquo;19</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/photometric_tracking_icra19/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/ICRA19_Bryner.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/ISgXVgCR-lE">
  Video
</a>






      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/esim_corl18/">
        <img src="/research/img/banners/esim_corl18_banner.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/esim_corl18/" itemprop="url">ESIM: an Open Event Camera Simulator</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Event cameras measure changes of intensity asynchronously, in the form of a stream of events, which encode per-pixel brightness changes. In the last few years, their outstanding properties (asynchronous sensing, no motion blur, high dynamic range) have led to exciting vision applications, with very low-latency and high robustness. However, these sensors are still scarce and expensive to get, slowing down progress of the research community. To address these issues, there is a huge demand for cheap, high-quality synthetic, labeled event for algorithm prototyping, deep learning and algorithm benchmarking. The development of such a simulator, however, is not trivial since event cameras work fundamentally differently from frame-based cameras. We present the first event camera simulator that can generate a large amount of reliable event data. The key component of our simulator is a theoretically sound, adaptive rendering scheme that only samples frames when necessary, through a tight coupling between the rendering engine and the event simulator. We release ESIM as open source.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Henri Rebecq, Daniel Gehrig, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>CoRL&rsquo;18</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/esim_corl18/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/CORL18_Rebecq.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/ytKOIX_2clo">
  Video
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/uzh-rpg/rpg_esim">
  Code
</a>





      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/malk_eccv18/">
        <img src="/research/img/banners/malk_eccv18_banner.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/malk_eccv18/" itemprop="url">Asynchronous, Photometric Feature Tracking using Events and Frames</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We present a method that leverages the complementarity of event cameras and standard cameras to track visual features with low latency. Event cameras are novel sensors that output pixel-level brightness changes, called &lsquo;events&rsquo;. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, because the same scene pattern can produce different events depending on the motion direction, establishing event correspondences across time is challenging. By contrast, standard cameras provide intensity measurements (frames) that do not depend on motion direction. Our method extracts features on frames and subsequently tracks them asynchronously using events, thereby exploiting the best of both types of data: the frames provide a photometric representation that does not depend on motion direction and the events provide low latency updates. In contrast to previous works, which are based on heuristics, this is the first principled method that uses raw intensity measurements directly, based on a generative event model within a maximum-likelihood framework. As a result, our method produces feature tracks that are both more accurate (subpixel accuracy) and longer than the state of the art, across a wide variety of scenes.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Daniel Gehrig, Henri Rebecq, Guillermo Gallego, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>ECCV&rsquo;18</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/malk_eccv18/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/ECCV18_Gehrig.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/A7UfeUnG6c4">
  Video
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/uzh-rpg/rpg_feature_tracking_analysis">
  Code
</a>





      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/stereo_eccv18/">
        <img src="/research/img/banners/stereo_eccv18_banner.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/stereo_eccv18/" itemprop="url">Semi-Dense 3D Reconstruction with a Stereo Event Camera</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        Event cameras are bio-inspired sensors that offer several advantages, such as low latency, high-speed and high dynamic range, to tackle challenging scenarios in computer vision. This paper presents a solution to the problem of 3D reconstruction from data captured by a stereo event-camera rig moving in a static scene, such as in the context of stereo Simultaneous Localization and Mapping. The proposed method consists of the optimization of an energy function designed to exploit small-baseline spatio-temporal consistency of events triggered across both stereo image planes. To improve the density of the reconstruction and to reduce the uncertainty of the estimation, a probabilistic depth-fusion strategy is also developed. The resulting method has no special requirements on either the motion of the stereo event-camera rig or on prior knowledge about the scene. Experiments demonstrate our method can deal with both texture-rich scenes as well as sparse scenes, outperforming state-of-the-art stereo methods based on event data image representations.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Yi Zhou, Guillermo Gallego, Henri Rebecq, Laurent Kneip, Hongdong Li, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>ECCV&rsquo;18</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/stereo_eccv18/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/ECCV18_Zhou.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/Qrnpj2FD1e4">
  Video
</a>






      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/contrast_max_cvpr/">
        <img src="/research/img/banners/contrast_max_banner.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/contrast_max_cvpr/" itemprop="url">A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth and Optical Flow Estimation</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We present a unifying framework to solve several computer vision problems with event cameras: motion, depth and optical flow estimation. The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events. Our method implicitly handles data association between the events, and therefore, does not rely on additional appearance information about the scene. The proposed method is not only simple, but more importantly, it is, to the best of our knowledge, the first method that can be successfully applied to such a diverse set of important vision tasks with event cameras.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Guillermo Gallego, Henri Rebecq, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>CVPR&rsquo;18</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/contrast_max_cvpr/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/CVPR18_Gallego.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/KFMZFhi-9Aw">
  Video
</a>






      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/emvs_ijcv/">
        <img src="/research/img/banners/emvs_banner.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/emvs_ijcv/" itemprop="url">EMVS: Event-based Multi-View Stereo - 3D Reconstruction with an Event Camera in Real-Time</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        In this paper, we introduce the problem of Event-based Multi-View Stereo (EMVS) for event cameras and propose a solution to it. Unlike traditional MVS methods, which address the problem of estimating dense 3D structure from a set of known viewpoints, EMVS estimates semi-dense 3D structure from an event camera with known trajectory. Our algorithm is able to produce accurate, semi-dense depth maps and is computationally very efficient (runs in real-time on a CPU or even a smartphone processor).
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Henri Rebecq, Guillermo Gallego, Elias Mueggler, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>IJCV&rsquo;17</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/emvs_ijcv/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/IJCV17_Rebecq.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/EFpZcpd9XJ0">
  Video
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/uzh-rpg/rpg_emvs">
  Code
</a>





      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/eb_6dof_tracking/">
        <img src="/research/img/banners/posetracking_banner.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/eb_6dof_tracking/" itemprop="url">Event-based, 6-DOF Camera Tracking from Photometric Depth Maps</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        In contrast to standard cameras, which produce frames at a fixed rate, event cameras respond asynchronously to pixel-level brightness changes, thus enabling the design of new algorithms for high-speed applications with latencies of microseconds. However, this advantage comes at a cost: because the output is composed by a sequence of events, traditional computer-vision algorithms are not applicable, so that a new paradigm shift is needed. We present an event-based approach for ego-motion estimation, which provides pose updates upon the arrival of each event, thus virtually eliminating latency. Our method is the first work addressing and demonstrating event-based pose tracking in six degrees-of-freedom (DOF) motions in realistic and natural scenes, and it is able to track high-speed motions. The method is successfully evaluated in both indoor and outdoor scenes.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Guillermo Gallego, Jon E.A. Lund, Elias Mueggler, Henri Rebecq, Tobi Delbruck, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>PAMI&rsquo;17</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/eb_6dof_tracking/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/PAMI17_Gallego.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://www.youtube.com/watch?v=iZZ77F-hwzs">
  Video
</a>






      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/zevio/">
        <img src="/research/img/banners/zevio.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/zevio/" itemprop="url">Real-time Visual-Inertial Odometry for Event Cameras using Keyframe-based Nonlinear Optimization</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        We propose a novel, accurate tightly-coupled visual-inertial odometry pipeline for such cameras that leverages the outstanding properties of event cameras to estimate the camera ego-motion in challenging conditions, such as high-speed motion or high dynamic range scenes. The method tracks a set of features (extracted on the image plane) through time. To achieve that, we consider events in overlapping spatio-temporal windows and align them using the current camera motion and scene structure, yielding motion-compensated event frames. We then combine these feature tracks in a keyframe-based, visual-inertial odometry algorithm based on nonlinear optimization to estimate the camera’s 6-DOF pose and velocity.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Henri Rebecq, Timo Horstschaefer, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>BMVC&rsquo;17</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/zevio/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/BMVC17_Rebecq.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/F3OFzsaPtvI">
  Video
</a>






      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/ultimate_slam/">
        <img src="/research/img/banners/ultimate_slam.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/ultimate_slam/" itemprop="url">Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenarios</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        In this paper, we present the first state estimation pipeline that leverages the complementary advantages of a standard camera and an event camera by fusing, in a tightly-coupled manner events, standard frames, and inertial measurements. Furthermore, we use our pipeline to demonstrate the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high-dynamic range scenes.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Antoni Rosinol Vidal, Henri Rebecq, Timo Horstschaefer, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>RA-L&rsquo;18</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/ultimate_slam/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/RAL18_VidalRebecq.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/jIvJuWdmemE">
  Video
</a>






      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/evo/">
        <img src="/research/img/banners/evo.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/evo/" itemprop="url">EVO: A Geometric Approach to Event-based 6-DOF Parallel Tracking and Mapping in Real-time</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        In this paper, we introduce the problem of Event-based Multi-View Stereo (EMVS) for event cameras and propose a solution to it. Unlike traditional MVS methods, which address the problem of estimating dense 3D structure from a set of known viewpoints, EMVS estimates semi-dense 3D structure from an event camera with known trajectory. Our algorithm is able to produce accurate, semi-dense depth maps and is computationally very efficient (runs in real-time on a CPU or even a smartphone processor).
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Henri Rebecq, Timo Horstschaefer, Guillermo Gallego, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>RA-L&rsquo;17</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/evo/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/RAL16_EVO.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/bYqD2qZJlxE">
  Video
</a>






      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/dvs_data_paper/">
        <img src="/research/img/papers/dvs_data_paper.jpg" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/dvs_data_paper/" itemprop="url">The Event-Camera Dataset and Simulator: Event-based Data for Pose Estimation, Visual Odometry, and SLAM</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        This presents the world&rsquo;s first collection of datasets with an event-based camera for high-speed robotics. The data also include intensity images, inertial measurements, and ground truth from a motion-capture system. An event-based camera is a revolutionary vision sensor with three key advantages: a measurement rate that is almost 1 million times faster than standard cameras, a latency of 1 microsecond, and a high dynamic range of 130 decibels (standard cameras only have 60 dB). These properties enable the design of a new class of algorithms for high-speed robotics, where standard cameras suffer from motion blur and high latency. All the data are released both as text files and binary (i.e., rosbag) files.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Elias Mueggler, Henri Rebecq, Guillermo Gallego, Tobi Delbruck, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>IJRR&rsquo;17</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/dvs_data_paper/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/pdf/1610.08336.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://www.youtube.com/watch?v=bVVBTQ7l36I">
  Video
</a>






      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/emvs/">
        <img src="/research/img/banners/emvs_bmvc_banner.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/emvs/" itemprop="url">EMVS: Event-based Multi-View Stereo</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        In this paper, we introduce the problem of Event-based Multi-View Stereo (EMVS) for event cameras and propose a solution to it. Unlike traditional MVS methods, which address the problem of estimating dense 3D structure from a set of known viewpoints, EMVS estimates semi-dense 3D structure from an event camera with known trajectory. Our algorithm is able to produce accurate, semi-dense depth maps and is computationally very efficient (runs in real-time on a CPU or even a smartphone processor).
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Henri Rebecq, Guillermo Gallego, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>BMVC&rsquo;16</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/emvs/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/BMVC16_Rebecq.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://www.youtube.com/watch?v=EUX3Tfx0KKE">
  Video
</a>


<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/uzh-rpg/rpg_emvs">
  Code
</a>





      </div>

    </div>
  </div>
</div>

    
    <div class="pub-list-item" itemscope itemtype="http://schema.org/CreativeWork">
  <div class="row">

    

    <div class="col-md-12">
      <a href="https://supitalp.github.io/research/publication/benefit-large-fov-vo/">
        <img src="/research/img/banners/benefit_fov_vo_banner.png" class="pub-banner" itemprop="image">
      </a>
    </div>
    <div class="col-md-12">

    

      <h3 class="article-title" itemprop="name">
        <a href="https://supitalp.github.io/research/publication/benefit-large-fov-vo/" itemprop="url">Benefit of Large Field-of-View Cameras for Visual Odometry</a>
      </h3>

      <div class="pub-abstract" itemprop="text">
        
        The transition of visual-odometry technology from research demonstrators to commercial applications naturally raises the question: &ldquo;what is the optimal camera for vision-based motion estimation?&rdquo; This question is crucial as the choice of camera has a tremendous impact on the robustness and accuracy of the employed visual odometry algorithm. While many properties of a camera (e.g. resolution, frame-rate, global-shutter/rolling-shutter) could be considered, in this work we focus on evaluating the impact of the camera field-of-view (FoV) and optics (i.e., fisheye or catadioptric) on the quality of the motion estimate. Since the motion-estimation performance depends highly on the geometry of the scene and the motion of the camera, we analyze two common operational environments in mobile robotics: an urban environment and an indoor scene.
        
      </div>

      <div class="pub-authors" itemprop="author">
        
        Zichao Zhang, Henri Rebecq, Christian Forster, Davide Scaramuzza
        
      </div>

      <div class="pub-publication">
        
        In <em>ICRA&rsquo;16</em>
        
      </div>

      <div class="pub-links">
        



<a class="btn btn-primary btn-outline btn-xs" href="https://supitalp.github.io/research/publication/benefit-large-fov-vo/">
  Details
</a>


<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/docs/ICRA16_Zhang.pdf">
  PDF
</a>



<a class="btn btn-primary btn-outline btn-xs" href="https://youtu.be/6KXBoprGaR0">
  Video
</a>





<a class="btn btn-primary btn-outline btn-xs" href="http://rpg.ifi.uzh.ch/fov.html">
  Research page
</a>


      </div>

    </div>
  </div>
</div>

    
    

  </div>
</div>

    </div>
  </section>

  
  
  <section id="teaching" class="home-section">
    <div class="container">
      


<div class="row">
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Teaching</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    <p>I am a teaching assistant for the course <a href="http://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheitPre.do?semkez=2016W&amp;lang=en&amp;ansicht=ALLE&amp;lerneinheitId=110044" target="_blank">Vision Algorithms for Mobile Robotics</a> given at ETH Zürich.</p>

<p>I also occasionally supervise student projects. The list of projects currently available can be found <a href="http://rpg.ifi.uzh.ch/student_projects.php" target="_blank">here</a>.</p>

  </div>
</div>

    </div>
  </section>

  
  
  <section id="contact" class="home-section">
    <div class="container">
      




<div class="row">
  <div class="col-xs-12 col-md-4 section-heading">
    <h1>Contact</h1>
    
  </div>
  <div class="col-xs-12 col-md-8">
    

    <ul class="fa-ul" itemscope>

      
      <li>
        <i class="fa-li fa fa-envelope fa-2x" aria-hidden="true"></i>
        <span id="person-email" itemprop="email"><a href="mailto:h.rebecq@gmail.com">h.rebecq@gmail.com</a></span>
      </li>
      

      

      

      

      

      
      <li>
        <i class="fa-li fa fa-map-marker fa-2x" aria-hidden="true"></i>
        <span id="person-address" itemprop="address">Robotics and Perception Group, Andreasstrasse 15, 8050 Zürich, Switzerland.</span>
      </li>
      

      

    </ul>

  </div>
</div>

    </div>
  </section>



<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2019 Henri Rebecq &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha512-jGsMH83oKe9asCpkOVkBnUrDDTp8wl+adkB2D+//JtlxO4SrLoJdhbOysIFQJloQFD+C4Fl1rMsQZF76JjV0eQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.2/imagesloaded.pkgd.min.js" integrity="sha512-iHzEu7GbSc705hE2skyH6/AlTpOfBmkx7nUqTLGzPYR+C1tRaItbRlJ7hT/D3YQ9SV0fqLKzp4XY9wKulTBGTw==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/TweenMax.min.js" integrity="sha512-Z5heTz36xTemt1TbtbfXtTq5lMfYnOkXM2/eWcTTiLU01+Sw4ku1i7vScDc8fWhrP2abz9GQzgKH5NGBLoYlAw==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/plugins/ScrollToPlugin.min.js" integrity="sha512-CDeU7pRtkPX6XJtF/gcFWlEwyaX7mcAp5sO3VIu/ylsdR74wEw4wmBpD5yYTrmMAiAboi9thyBUr1vXRPA7t0Q==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    <script src="/research/js/hugo-academic.js"></script>
    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>

